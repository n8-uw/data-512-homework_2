{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Considering Bias in Data\n",
    "This notebook will explore the bias in Wikipedias page rating system. Using article data of various political leaders from all around the world, we query the [ORES system](https://ores.wikimedia.org/) in order to obtain rating estimates. \n",
    "\n",
    "## License\n",
    "This code example was developed by Nathan Grant for use in DATA 512, a course in the UW MS Data Science degree program. This code is provided un\n",
    "der the [Creative Commons](https://creativecommons.org) [CC-BY license](https://creativecommons.org/licenses/by/4.0/). Revision 1.0 - May 13, 2022\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T08:08:37.905044Z",
     "start_time": "2022-10-11T08:08:36.828933Z"
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# These are standard python modules\n",
    "import json, time, urllib.parse, os\n",
    "#\n",
    "# The 'requests' module is not a standard Python module. You will need to install this with pip/pip3 if you do not already have it\n",
    "import requests\n",
    "# The 'pandas' module is not a standard Python module. You will need to install this with pip/pip3 if you do not already have it\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Data Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T08:08:38.643020Z",
     "start_time": "2022-10-11T08:08:38.600003Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Shahjahan Noori</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Shahjahan_Noori</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Abdul Ghafar Lakanwal</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abdul_Ghafar_Lak...</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Majah Ha Adrif</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Majah_Ha_Adrif</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Haroon al-Afghani</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Haroon_al-Afghani</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Tayyab Agha</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Tayyab_Agha</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name                                                url  \\\n",
       "0        Shahjahan Noori      https://en.wikipedia.org/wiki/Shahjahan_Noori   \n",
       "1  Abdul Ghafar Lakanwal  https://en.wikipedia.org/wiki/Abdul_Ghafar_Lak...   \n",
       "2         Majah Ha Adrif       https://en.wikipedia.org/wiki/Majah_Ha_Adrif   \n",
       "3      Haroon al-Afghani    https://en.wikipedia.org/wiki/Haroon_al-Afghani   \n",
       "4            Tayyab Agha          https://en.wikipedia.org/wiki/Tayyab_Agha   \n",
       "\n",
       "       country  \n",
       "0  Afghanistan  \n",
       "1  Afghanistan  \n",
       "2  Afghanistan  \n",
       "3  Afghanistan  \n",
       "4  Afghanistan  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handle data inconsistencies in wikipedia data\n",
    "politicians_dataframe = pd.read_csv(os.getcwd()+ '\\\\data\\\\politicians_by_country_SEPT2022.csv')\n",
    "politicians_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T08:08:39.583186Z",
     "start_time": "2022-10-11T08:08:39.564187Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Torokul Dzhanuzakov      4\n",
       "Alexandra Benado         2\n",
       "Konstantin Jireček       2\n",
       "Zahur Ahmad Chowdhury    2\n",
       "Visar Ymeri              2\n",
       "                        ..\n",
       "Jhajaira Urresta         1\n",
       "Ferenc Pulszky           1\n",
       "Chun Yung-woo            1\n",
       "Pēteris Pētersons        1\n",
       "Hassan Adan Wadadid      1\n",
       "Name: name, Length: 7534, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicate records\n",
    "politicians_dataframe['name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T08:08:39.993431Z",
     "start_time": "2022-10-11T08:08:39.972411Z"
    }
   },
   "outputs": [],
   "source": [
    "politicians_dataframe = politicians_dataframe.drop_duplicates(subset='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T08:08:40.512145Z",
     "start_time": "2022-10-11T08:08:40.496164Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Geography</th>\n",
       "      <th>Population (millions)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>WORLD</td>\n",
       "      <td>7963.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>1419.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>NORTHERN AFRICA</td>\n",
       "      <td>251.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>44.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>103.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Geography  Population (millions)\n",
       "0            WORLD                 7963.0\n",
       "1           AFRICA                 1419.0\n",
       "2  NORTHERN AFRICA                  251.0\n",
       "3          Algeria                   44.9\n",
       "4            Egypt                  103.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "population_dataframe = pd.read_csv(os.getcwd()+ '\\\\data\\\\population_by_country_2022.csv')\n",
    "population_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T08:08:40.968178Z",
     "start_time": "2022-10-11T08:08:40.947161Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get rows that are regions instead of countries\n",
    "region_mask = population_dataframe['Geography'].str.isupper()\n",
    "regions = population_dataframe[region_mask]\n",
    "\n",
    "# Isolate countries\n",
    "countries = population_dataframe[~region_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining revision ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T08:08:42.040211Z",
     "start_time": "2022-10-11T08:08:42.020192Z"
    }
   },
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "# The basic English Wikipedia API endpoint\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "# We'll assume that there needs to be some throttling for these requests - we should always be nice to a free data resource\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making automated requests we should include something that is unique to the person making the request\n",
    "# This should include an email - your UW email would be good to put in there\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<uwnetid@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2022',\n",
    "}\n",
    "\n",
    "# This is a string of additional page properties that can be returned see the Info documentation for\n",
    "# what can be included. If you don't want any this can simply be the empty string\n",
    "PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers\"\n",
    "#PAGEINFO_EXTENDED_PROPERTIES = \"\"\n",
    "\n",
    "# This template lists the basic parameters for making this\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",           # to simplify this should be a single page title at a time\n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API request will be made using one procedure. The idea is to make this reusable. The procedure is parameterized, but relies on the constants above for the important parameters. The underlying assumption is that this will be used to request data for a set of article pages. Therefore the parameter most likely to change is the article_title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T08:08:43.852395Z",
     "start_time": "2022-10-11T08:08:43.835396Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_pageinfo_per_article(article_title = None, \n",
    "                                 endpoint_url = API_ENWIKIPEDIA_ENDPOINT, \n",
    "                                 request_template = PAGEINFO_PARAMS_TEMPLATE,\n",
    "                                 headers = REQUEST_HEADERS):\n",
    "    # Make sure we have an article title\n",
    "    if not article_title: return None\n",
    "    \n",
    "    request_template['titles'] = article_title\n",
    "        \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or any other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T08:39:55.444634Z",
     "start_time": "2022-10-11T08:08:53.975613Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: 'C:\\\\Users\\\\NathanGrant/.netrc'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-5a748af73fee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mresponses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpoliticians_dataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mresponses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest_pageinfo_per_article\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-72d051e7e077>\u001b[0m in \u001b[0;36mrequest_pageinfo_per_article\u001b[1;34m(article_title, endpoint_url, request_template, headers)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mAPI_THROTTLE_WAIT\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAPI_THROTTLE_WAIT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendpoint_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest_template\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mjson_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    514\u001b[0m             \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m         )\n\u001b[1;32m--> 516\u001b[1;33m         \u001b[0mprep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[0mproxies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproxies\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mprepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    444\u001b[0m         \u001b[0mauth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrust_env\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mauth\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m             \u001b[0mauth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_netrc_auth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPreparedRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\requests\\utils.py\u001b[0m in \u001b[0;36mget_netrc_auth\u001b[1;34m(url, raise_errors)\u001b[0m\n\u001b[0;32m    184\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m                 \u001b[0mnetrc_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Gather wiki responses for each politician and add them to a list\n",
    "responses = []\n",
    "for name in politicians_dataframe['name']:\n",
    "    responses.append(request_pageinfo_per_article(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T08:39:55.507963Z",
     "start_time": "2022-10-11T08:39:55.448641Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filter responses and display unscessful queries\n",
    "unsuccessful_queries = []\n",
    "successful_queries = []\n",
    "for r in responses:\n",
    "    if \"lastrevid\" not in r['query']['pages'][list(r['query']['pages'].keys())[0]]:\n",
    "        unsuccessful_queries.append(r['query']['pages'][list(r['query']['pages'].keys())[0]]['title'])\n",
    "    else:\n",
    "        data = r['query']['pages'][list(r['query']['pages'].keys())[0]]\n",
    "        successful_queries.append((data['title'],data['lastrevid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T08:39:55.523998Z",
     "start_time": "2022-10-11T08:39:55.511980Z"
    }
   },
   "outputs": [],
   "source": [
    "unsuccessful_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining page ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the wp10 model because its the only model that had a wiki outlining its [performance](https://meta.wikimedia.org/wiki/Objective_Revision_Evaluation_Service/wp10) on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T08:39:55.539967Z",
     "start_time": "2022-10-11T08:39:55.527996Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "# The current ORES API endpoint\n",
    "API_ORES_SCORE_ENDPOINT = \"https://ores.wikimedia.org/v3\"\n",
    "# A template for mapping to the URL\n",
    "API_ORES_SCORE_PARAMS = \"/scores/{context}/{revid}/{model}\"\n",
    "\n",
    "# Use some delays so that we do not hammer the API with our requests\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making automated requests we should include something that is unique to the person making the request\n",
    "# This should include an email - your UW email would be good to put in there\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<uwnetid@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2022'\n",
    "}\n",
    "\n",
    "# A dictionary of English Wikipedia article titles (keys) and sample revision IDs that can be used for this ORES scoring example\n",
    "ARTICLE_REVISIONS = { 'Bison':1085687913 , 'Northern flicker':1086582504 , 'Red squirrel':1083787665 , 'Chinook salmon':1085406228 , 'Horseshoe bat':1060601936 }\n",
    "\n",
    "# This template lists the basic parameters for making an ORES request\n",
    "ORES_PARAMS_TEMPLATE = {\n",
    "    \"context\": \"enwiki\",        # which WMF project for the specified revid\n",
    "    \"revid\" : \"\",               # the revision to be scored - this will probably change each call\n",
    "    \"model\": \"wp10\"   # the AI/ML scoring model to apply to the reviewion\n",
    "}\n",
    "#\n",
    "# The current ML models for English wikipedia are:\n",
    "#   \"articlequality\"\n",
    "#   \"articletopic\"\n",
    "#   \"damaging\"\n",
    "#   \"version\"\n",
    "#   \"draftquality\"\n",
    "#   \"drafttopic\"\n",
    "#   \"goodfaith\"\n",
    "#   \"wp10\"\n",
    "#\n",
    "# The specific documentation on these is scattered so if you want to use one you'll have to look around.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T08:39:55.555966Z",
     "start_time": "2022-10-11T08:39:55.542996Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_ores_score_per_article(article_revid = None, \n",
    "                                   endpoint_url = API_ORES_SCORE_ENDPOINT, \n",
    "                                   endpoint_params = API_ORES_SCORE_PARAMS, \n",
    "                                   request_template = ORES_PARAMS_TEMPLATE,\n",
    "                                   headers = REQUEST_HEADERS,\n",
    "                                   features=False):\n",
    "    # Make sure we have an article revision id\n",
    "    if not article_revid: return None\n",
    "    \n",
    "    # set the revision id into the template\n",
    "    request_template['revid'] = article_revid\n",
    "    \n",
    "    # now, create a request URL by combining the endpoint_url with the parameters for the request\n",
    "    request_url = endpoint_url+endpoint_params.format(**request_template)\n",
    "    \n",
    "    # the features used by the ML model can sometimes be returned as well as scores\n",
    "    if features:\n",
    "        request_url = request_url+\"?features=true\"\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like ORES - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T09:35:54.798104Z",
     "start_time": "2022-10-11T08:39:55.557966Z"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate through all of the revids and get the article scores\n",
    "score_responses = []\n",
    "for name, revid in successful_queries:\n",
    "    response = request_ores_score_per_article(revid)\n",
    "    score_responses.append((name, response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example score response\n",
    "score_responses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T09:35:54.814101Z",
     "start_time": "2022-10-11T09:35:54.802076Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parse score responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T09:35:54.956072Z",
     "start_time": "2022-10-11T09:35:54.820102Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filter through the responses to determine which ones were valid\n",
    "unsuccessful_score_queries = []\n",
    "successful_score_queries = []\n",
    "for name, res in score_responses:\n",
    "    revid = list(res['enwiki']['scores'].keys())[0]\n",
    "    # If the wp10 element is in the response dictionary\n",
    "    if 'wp10' in res['enwiki']['scores'][revid]:\n",
    "        score = res['enwiki']['scores'][revid]['wp10']['score']\n",
    "        # If prediction isnt in the json\n",
    "        if 'prediction' not in score:\n",
    "            # Add it to unsuccessful queries\n",
    "            unsuccessful_score_queries.append(name)\n",
    "        else:\n",
    "            #  Else add it to successful queries\n",
    "            if score['prediction'] not \"NaN\":\n",
    "                successful_score_queries.append((name,revid,score['prediction']))\n",
    "            else:\n",
    "                unsuccessful_score_queries.append(name)\n",
    "    else:\n",
    "        unsuccessful_score_queries.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T09:35:54.959098Z",
     "start_time": "2022-10-11T08:09:03.232Z"
    }
   },
   "outputs": [],
   "source": [
    "# No unsuccessful queries\n",
    "len(unsuccessful_score_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T09:35:54.960073Z",
     "start_time": "2022-10-11T08:09:04.120Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dataframe of the successful queries with their scores\n",
    "score_df = pd.DataFrame(data=successful_score_queries,columns=['name','revid','score'])\n",
    "score_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T09:35:54.962073Z",
     "start_time": "2022-10-11T08:09:05.011Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove politicians whos records could not be resolved\n",
    "politicians_dataframe = politicians_dataframe[~politicians_dataframe[\"name\"].isin(unsuccessful_queries)]\n",
    "politicians_dataframe.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T09:35:54.963072Z",
     "start_time": "2022-10-11T08:09:05.387Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combine the politician table with the articles scores\n",
    "merged_df = pd.merge(score_df,politicians_dataframe, how='inner', on='name')\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scheme to generate regions from the countries\n",
    "geography_map = []\n",
    "levels = []\n",
    "\n",
    "# Filter through countries regions\n",
    "for g,pop in population_dataframe.values:\n",
    "    # If its upper case then add it to the stack of region names\n",
    "    if g.isupper():\n",
    "        levels.append(g)\n",
    "    # If its not then get the last region on the stack and add it to the list\n",
    "    else:\n",
    "        geography_map.append([g,levels[-1],pop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example country tuple\n",
    "geography_map[0], geography_map[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crete a datafraome of the countries with their regions and populations\n",
    "region_df = pd.DataFrame(data=geography_map,columns=[\"country\",\"region\",\"population\"])\n",
    "region_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the regions and populations with the article ratings table\n",
    "final_df = pd.merge(region_df,merged_df, how='outer', on='country')\n",
    "# Select necessary columns\n",
    "final_df = final_df[[\"country\",\"region\",\"population\",\"name\",\"revid\",\"score\"]]\n",
    "# Reset the column names to the desired table output\n",
    "final_df.columns = [\"country\",\"region\",\"population\",\"article_title\",\"revision_id\",\"article_quality\"]\n",
    "# Remove rows that have NAs\n",
    "has_nans = final_df[final_df['population'].isna()]\n",
    "# Final df is all articles that a rating could be generated for\n",
    "final_df = final_df[~final_df['population'].isna()]\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk in the working folder\n",
    "final_df.to_csv(os.getcwd()+\"\\\\wp_politicians_by_country.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out which countries couldnt be resolved\n",
    "all_nans = has_nans['country'].unique()\n",
    "all_nans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here there is no data for Korean because in the population CSV, the countries are represented as Korea, North and Korea, South so therefore the table operations result in no key matches. For this reason we will not include these two countries in the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save contries that dont have a match to a text tfile\n",
    "with open(os.getcwd()+\"\\\\wp_countries-no_match.txt\",'a') as f:\n",
    "    for c in all_nans:\n",
    "        f.write(c+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will calculate total-articles-per-population (a ratio representing the number of articles per person) and high-quality-articles-per-population (a ratio representing the number of high quality articles per person) on a country-by-country and regional basis. Also we will normalize them all to be “per capita”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 countries by coverage: \n",
    "- The 10 countries with the highest total articles per capita (in descending order):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group articles together by country and get a count of articles\n",
    "total_articles_per_country = final_df.groupby(\"country\")['article_title'].count()\n",
    "total_articles_per_country.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge together the articles for each country with countries populations\n",
    "total_articles_per_pop_country = pd.merge(total_articles_per_country,countries, \n",
    "                                          how=\"inner\",left_index=True ,right_on=\"Geography\")\n",
    "# Create a new column that is the number of articles divided by the normalized population\n",
    "total_articles_per_pop_country[\"articles_per_population_mil\"] = total_articles_per_pop_country[\"article_title\"]/\\\n",
    "                                                        total_articles_per_pop_country[\"Population (millions)\"]\n",
    "# Select desired columns select the largest and sort in descending order\n",
    "total_articles_per_pop_country_highest = total_articles_per_pop_country[[\"Geography\",\"articles_per_population_mil\"]].sort_values(ascending=False, \n",
    "                                        by =\"articles_per_population_mil\")\n",
    "# Display the top 10\n",
    "total_articles_per_pop_country_highest.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bottom 10 countries by coverage: \n",
    "- The 10 countries with the lowest total articles per capita (in ascending order) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select desired columns select the largest and sort in ascending order\n",
    "total_articles_per_pop_country_lowest = total_articles_per_pop_country[[\"Geography\",\"articles_per_population_mil\"]]. \\\n",
    "                                        sort_values(ascending=False, \n",
    "                                        by =\"articles_per_population_mil\")\n",
    "total_articles_per_pop_country_lowest.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 countries by high quality:\n",
    "- The 10 countries with the highest high quality articles per capita (in descending order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter articles that are rated FA or GA by the model\n",
    "only_quality_articles = final_df[(final_df['score'] == \"FA\") | (final_df['score'] == \"GA\")]\n",
    "only_quality_articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge together the articles for each country with countries populations\n",
    "total_qual_articles_per_pop_country = pd.merge(only_quality_articles,countries, \n",
    "                                          how=\"inner\",left_index=True ,right_on=\"Geography\")\n",
    "# Create a new column that is the number of articles divided by the normalized population\n",
    "total_qual_articles_per_pop_country[\"articles_per_population_mil\"] = total_qual_articles_per_pop_country[\"article_title\"]/\\\n",
    "                                                        total_qual_articles_per_pop_country[\"Population (millions)\"]\n",
    "# Select desired columns select the largest and sort in descending order\n",
    "total_qual_articles_per_pop_country_high = total_qual_articles_per_pop_country[[\"Geography\",\n",
    "                                        \"articles_per_population_mil\"]].sort_values(ascending=False, \n",
    "                                        by =\"articles_per_population_mil\")\n",
    "total_qual_articles_per_pop_country_high.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bottom 10 countries by high quality: \n",
    "- The 10 countries with the lowest high quality articles per capita (in ascending order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select desired columns select the largest and sort in ascending order\n",
    "total_qual_articles_per_pop_country_low = total_qual_articles_per_pop_country[[\"Geography\",\n",
    "                                        \"articles_per_population_mil\"]].sort_values(ascending=True, \n",
    "                                        by =\"articles_per_population_mil\")\n",
    "total_qual_articles_per_pop_country_low.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geographic regions by total coverage: \n",
    "- A rank ordered list of geographic regions (in descending order) by total articles per capita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate counts together by region\n",
    "article_counts_region = final_df.groupby(\"region\")['article_quality'].count()\n",
    "article_counts_region.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge together the articles for each region with regions populations\n",
    "total_articles_per_pop_reg = pd.merge(article_counts_region,regions, \n",
    "                                          how=\"inner\",left_index=True ,right_on=\"Geography\")\n",
    "# Create a new column that is the number of articles divided by the normalized population\n",
    "total_articles_per_pop_reg[\"articles_per_population_mil\"] = total_articles_per_pop_reg[\"article_quality\"]/\\\n",
    "                                                        total_articles_per_pop_reg[\"Population (millions)\"]\n",
    "# Select desired columns and sort values in descending order\n",
    "total_articles_per_pop_reg = total_articles_per_pop_reg[[\"Geography\",\n",
    "                                        \"articles_per_population_mil\"]].sort_values(ascending=False, \n",
    "                                        by =\"articles_per_population_mil\")\n",
    "# Set column names\n",
    "total_articles_per_pop_reg.columns = [\"country\",\"articles_per_population_mil\"]\n",
    "# Display\n",
    "total_articles_per_pop_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geographic regions by high quality coverage: \n",
    "- Rank ordered list of geographic regions (in descending order) by high quality articles per capita.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate counts together by region\n",
    "article_counts_region_good = only_quality_articles.groupby(\"region\")['article_quality'].count()\n",
    "# Display\n",
    "article_counts_region_good.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge together the articles for each region with regions populations\n",
    "total_articles_per_pop_reg_good = pd.merge(article_counts_region_good,regions, \n",
    "                                          how=\"inner\",left_index=True ,right_on=\"Geography\")\n",
    "# Create a new column that is the number of articles divided by the normalized population\n",
    "total_articles_per_pop_reg_good[\"articles_per_population_mil\"] = total_articles_per_pop_reg_good[\"article_quality\"]/\\\n",
    "                                                        total_articles_per_pop_reg_good[\"Population (millions)\"]\n",
    "# Select desired columns and sort values in descending order\n",
    "total_articles_per_pop_reg_good = total_articles_per_pop_reg_good[[\"Geography\",\n",
    "                                        \"articles_per_population_mil\"]].sort_values(ascending=False, \n",
    "                                        by =\"articles_per_population_mil\")\n",
    "# Set column names\n",
    "total_articles_per_pop_reg_good.columns = [\"country\",\"articles_per_population_mil\"]\n",
    "total_articles_per_pop_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
